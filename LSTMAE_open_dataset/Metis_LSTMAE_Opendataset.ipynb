{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Metis-LSTMAE-Opendataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGGk-atVA_SD",
        "outputId": "7db324dd-345d-40f6-8e52-392c872f876b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "#Create time series to feed into autoencoder\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import normalize\n",
        "from pandas import read_csv\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "#Create 3d array\n",
        "def create_time_series(filepath, n_timesteps):\n",
        "\n",
        "    time_series = []\n",
        "    Y = []\n",
        "    first_y = True\n",
        "    first_windows = True\n",
        "    for filename in os.listdir(filepath):\n",
        "        print(filename)\n",
        "        data = np.load(filepath + \"/\" + filename)\n",
        "        data = normalize(data, axis=1)\n",
        "        windows = create_sliding_windows(data, n_timesteps)\n",
        "        y_category = create_Y_for_category(filename, windows.shape[0])\n",
        "        if first_y:\n",
        "            Y = np.array(y_category)\n",
        "            first_y = False\n",
        "        else:\n",
        "            Y = np.vstack((Y,y_category))\n",
        "\n",
        "        if first_windows:\n",
        "            time_series = windows\n",
        "            first_windows = False\n",
        "        else:\n",
        "            time_series = np.vstack((time_series, windows))\n",
        "\n",
        "    return time_series, Y\n",
        "\n",
        "\n",
        "\n",
        "# Create windows of n_timesteps BY ADDING PADDING AND THEN SPLITTING THE ARRAY\n",
        "def split_into_windows(X, n_timesteps):\n",
        "\n",
        "    windows = []\n",
        "\n",
        "    # Fill array until it can be split into windows of 10 timesteps (elements)\n",
        "    while len(X) % n_timesteps != 0:\n",
        "        X = np.row_stack((X, X[ len(X) -1 ]))\n",
        "\n",
        "    windows = np.array_split(X, (len(X) / n_timesteps), axis=0)\n",
        "    return np.array(windows)\n",
        "\n",
        "\n",
        "#Create y data for one category\n",
        "def create_Y_for_category(filename, windows_length):\n",
        "\n",
        "    indexLabel = findIndexLabel(filename)\n",
        "    Y = np.full((windows_length, 1), indexLabel)\n",
        "    return Y\n",
        "\n",
        "\n",
        "# Find index of the label assigned to the exercise\n",
        "def findIndexLabel(filename):\n",
        "    categories = load_txt(\"/content/drive/My Drive/Colab Notebooks/categories\")\n",
        "    for index in range(len(categories)):\n",
        "        if filename.split(\".\")[0]  == categories[index]:\n",
        "            return index\n",
        "\n",
        "\n",
        "# load a single file txt as a numpy array\n",
        "def load_txt(filepath):\n",
        "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "    return dataframe.values\n",
        "\n",
        "\n",
        "# Create sliding windows of n_timesteps (not used)\n",
        "def create_sliding_windows(data, n_timesteps):\n",
        "\n",
        "    windows = []\n",
        "    for i in range(data.shape[0] - n_timesteps):\n",
        "        windows.append(data[i: i + n_timesteps])\n",
        "\n",
        "    return np.array(windows)\n",
        "\n",
        "\n",
        "\n",
        "#LSTM-Autoencoder Training\n",
        "\n",
        "#Big function to extract features\n",
        "def extract_features(filepath):\n",
        "    n_timesteps = 60 #CHANGE TIMESTEPS HERE\n",
        "    X, Y = create_time_series(filepath, n_timesteps)\n",
        "    print(X.shape)\n",
        "    print(Y.shape)\n",
        "    X_encoded = evaluate_AE_model(X) #Evaluate model and extract features\n",
        "    print(X_encoded.shape)\n",
        "    return X_encoded, Y\n",
        "\n",
        "\n",
        "# Create, train and evaluate model\n",
        "def evaluate_AE_model(X):\n",
        "\n",
        "    print(\"TRAINING LSTM AUTOENCODER\")\n",
        "\n",
        "    #plt.plt(X[0])\n",
        "    n_timesteps = X.shape[1] \n",
        "    n_features = X.shape[2]\n",
        "   \n",
        "    #create encoder part\n",
        "    encoder_decoder = Sequential()\n",
        "    encoder_decoder.add(LSTM(128, activation='relu', input_shape=(n_timesteps, n_features), return_sequences=True))\n",
        "    encoder_decoder.add(LSTM(64, activation='relu', input_shape=(n_timesteps, n_features), return_sequences=False))\n",
        "    ## Le nombre de features extrait depend en fait du nombre de neurones du dernier layer de l encodeur \n",
        "\n",
        "    #create decoder part\n",
        "    encoder_decoder.add(RepeatVector(n_timesteps))\n",
        "    encoder_decoder.add(LSTM(64, activation='relu', return_sequences=True))\n",
        "    encoder_decoder.add(LSTM(128, activation='relu', return_sequences=True))\n",
        "    encoder_decoder.add(TimeDistributed(Dense(n_features)))\n",
        "\n",
        "    n_epochs = 5\n",
        "    encoder_decoder.compile(optimizer= \"adam\", loss='mse')\n",
        "    encoder_decoder.fit(X, X, epochs=n_epochs,verbose=1, batch_size=100)\n",
        "\n",
        "    encoder_decoder.save(\"/content/drive/My Drive/Colab Notebooks/LSTMAE_\" + str(n_timesteps) + \"timesteps_\" + str(n_epochs) + \"_sw\" )\n",
        "    # X_pred = encoder_decoder.predict(X)\n",
        "    # plt.plot(X_pred[0])\n",
        "\n",
        "    #Extract features\n",
        "    encoder = Model(inputs=encoder_decoder.inputs, outputs=encoder_decoder.layers[1].output)\n",
        "    X_encoded = encoder.predict(X)\n",
        "\n",
        "    return X_encoded\n",
        "\n",
        "\n",
        "\n",
        "# fit and evaluate DNN or RF\n",
        "def evaluate_model(X_train, y_train, X_test, y_test):\n",
        "\n",
        "    #Neural network\n",
        "\n",
        "    print(\"TRAINING CLASSIFIER\")\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    n_epochs = 20\n",
        "    model.fit(X_train, y_train, epochs=n_epochs, batch_size=100)\n",
        "\n",
        "    _, accuracy = model.evaluate(X_test, y_test,verbose=1)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    model.save(\"/content/drive/My Drive/Colab Notebooks/DNN_\" + str(n_epochs) + \"_\" + str(accuracy * 100) + \"_sw\")\n",
        "\n",
        "    print('Accuracy: %.2f' % (accuracy*100))\n",
        "\n",
        "\n",
        "    y_pred = np.argmax(y_pred,axis = 1) \n",
        "    y_test = np.argmax(y_test,axis = 1)\n",
        "    print(y_pred.shape)\n",
        "    print(y_test.shape)\n",
        "    print(confusion_matrix(y_pred, y_test))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    # RANDOM FOREST\n",
        "    # RF = RandomForestClassifier(n_estimators=250, max_depth=2,\n",
        "    #                          random_state=3)\n",
        "    # RF.fit(X_train, y_train)\n",
        "    # score = RF.score(X_test, y_test)\n",
        "    # print(score)\n",
        "   \n",
        "# main function to call\n",
        "def run_experiment():\n",
        "    file_name = \"/content/drive/My Drive/Colab Notebooks/New Dataset\"\n",
        "    X_data, Y_data = extract_features(file_name)\n",
        "\n",
        "    Y_data = to_categorical(Y_data)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_data, Y_data, stratify=Y_data,test_size=0.30, random_state=42\n",
        "    )\n",
        "\n",
        "    evaluate_model(X_train, y_train, X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "run_experiment()\n",
        "   \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HandOpen.npy\n",
            "HandRest.npy\n",
            "ObjectGrip.npy\n",
            "PichGrip.npy\n",
            "WristPron.npy\n",
            "WristSupi.npy\n",
            "WristExten.npy\n",
            "WristFlex.npy\n",
            "(1659520, 60, 7)\n",
            "(1659520, 1)\n",
            "TRAINING LSTM AUTOENCODER\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r12JQqZ2zGQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}